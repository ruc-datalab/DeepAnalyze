#!/usr/bin/env python3
"""
Example usage of DeepAnalyze OpenAI-Compatible API
Demonstrates common use cases including 2-turn data analysis with file attachments
"""

import requests
import time
import json

API_BASE = "http://localhost:8200"
MODEL = "DeepAnalyze-8B"


def simple_chat():
    """Simple chat without files"""
    response = requests.post(f"{API_BASE}/v1/chat/completions", json={
        "model": MODEL,
        "messages": [
            {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»Pythonç¼–ç¨‹è¯­è¨€"}
        ],
        "temperature": 0.3
    })

    if response.status_code == 200:
        result = response.json()
        content = result['choices'][0]['message']['content']
        print(f"Assistant: {content[:100]}...")
    else:
        print(f"âŒ Error: {response.text}")


def chat_with_file():
    """Chat with file attachment"""
    # Upload file
    with open("./Simpson.csv", 'rb') as f:
        files = {'file': ('Simpson.csv', f, 'text/csv')}
        data = {'purpose': 'file-extract'}
        response = requests.post(f"{API_BASE}/v1/files", files=files, data=data)

    if response.status_code != 200:
        print(f"âŒ Upload failed: {response.text}")
        return

    file_id = response.json()['id']

    # Chat with file
    response = requests.post(f"{API_BASE}/v1/chat/completions", json={
        "model": MODEL,
        "messages": [
            {"role": "user", "content": "åˆ†æå“ªç§æ•™å­¦æ–¹æ³•æ•ˆæœæ›´å¥½ã€‚"}
        ],
        "file_ids": [file_id],
        "temperature": 0.3
    })

    if response.status_code == 200:
        result = response.json()
        content = result['choices'][0]['message']['content']
        print(f"Response: {content}...")

        files = result.get('generated_files', [])
        if files:
            print(f"Files: {len(files)} generated")

    # Cleanup
    # requests.delete(f"{API_BASE}/v1/files/{file_id}")




def file_ids_in_messages():
    """Chat completion with file_ids in messages (OpenAI compatibility)"""
    # Upload file
    with open("./Simpson.csv", 'rb') as f:
        files = {'file': ('Simpson.csv', f, 'text/csv')}
        data = {'purpose': 'file-extract'}
        response = requests.post(f"{API_BASE}/v1/files", files=files, data=data)

    file_id = response.json()['id']

    # New format: file_ids in messages
    response = requests.post(f"{API_BASE}/v1/chat/completions", json={
        "model": MODEL,
        "messages": [
            {
                "role": "user",
                "content": "åˆ†ææ•°æ®å¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ã€‚",
                "file_ids": [file_id]  # can both be inside message and top-level, for compatibility with OpenAI API
            }
        ],
        "temperature": 0.3
    })

    if response.status_code == 200:
        result = response.json()
        content = result['choices'][0]['message']['content']
        print(f"Response: {content[:100]}...")

        # Check for files in message (new format)
        message = result['choices'][0]['message']
        if 'files' in message:
            print(f"Files in message: {len(message['files'])}")

        # Check for generated_files (backward compatibility)
        if 'generated_files' in result:
            print(f"Generated files: {len(result['generated_files'])}")

    # Cleanup
    # requests.delete(f"{API_BASE}/v1/files/{file_id}")


def streaming_chat():
    """Streaming chat response"""
    # Upload file
    with open("./Simpson.csv", 'rb') as f:
        files = {'file': ('Simpson.csv', f, 'text/csv')}
        data = {'purpose': 'file-extract'}
        response = requests.post(f"{API_BASE}/v1/files", files=files, data=data)

    file_id = response.json()['id']

    print("Streaming response...")
    response = requests.post(
        f"{API_BASE}/v1/chat/completions",
        json={
            "model": MODEL,
            "messages": [
                {
                    "role": "user",
                    "content": "åˆ†ææ•°æ®å¹¶ç”Ÿæˆè¶‹åŠ¿å›¾ã€‚",
                    "file_ids": [file_id]
                }
            ],
            "temperature": 0.3,
            "stream": True
        },
        stream=True
    )

    if response.status_code == 200:
        for line in response.iter_lines():
            if line:
                line_str = line.decode('utf-8')
                if line_str.startswith('data: '):
                    data_str = line_str[6:]
                    if data_str == '[DONE]':
                        break
                    try:
                        chunk = json.loads(data_str)
                        if 'choices' in chunk and chunk['choices']:
                            delta = chunk['choices'][0].get('delta', {})
                            if 'content' in delta:
                                print(delta['content'], end='', flush=True)
                        if 'generated_files' in chunk:
                            print(f"\n\nğŸ“ New file generated: {chunk['generated_files']}")
                    except json.JSONDecodeError:
                        pass
        print("\nâœ… Streaming complete")

    


def check_server():
    """Check if API server is running"""
    try:
        response = requests.get(f"{API_BASE}/health", timeout=2)
        return response.status_code == 200
    except:
        return False


def multi_turn_example():
    """Demonstrate thread_id workflow for 2-turn data analysis conversation with streaming and file attachments"""
    print("ğŸ§µ Testing Thread ID Workflow with Streaming for Data Analysis...")

    conversation_history = []

    # Upload data file
    with open("./Simpson.csv", 'rb') as f:
        files = {'file': ('Simpson.csv', f, 'text/csv')}
        data = {'purpose': 'file-extract'}
        response = requests.post(f"{API_BASE}/v1/files", files=files, data=data)

    if response.status_code != 200:
        print(f"âŒ File upload failed: {response.text}")
        return

    file_id = response.json()['id']

    # First request - creates new thread, examine data structure
    print("\n1ï¸âƒ£ First request - examining data structure...")
    conversation_history.append({
        "role": "user",
        "content": "è¯·æŸ¥çœ‹è¿™ä¸ªæ•°æ®æ–‡ä»¶çš„ç»“æ„ï¼Œå‘Šè¯‰æˆ‘æœ‰å“ªäº›å­—æ®µã€æ•°æ®ç±»å‹å’ŒåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ã€‚"
    })

    print("Streaming response...")
    response = requests.post(
        f"{API_BASE}/v1/chat/completions",
        json={
            "model": MODEL,
            "messages": conversation_history,
            "file_ids": [file_id],
            "temperature": 0.3,
            "stream": True
        },
        stream=True
    )

    if response.status_code != 200:
        print(f"âŒ First request failed: {response.text}")
        return

    full_response = ""
    received_thread_id = None
    generated_files = []

    for line in response.iter_lines():
        if line:
            line_str = line.decode('utf-8')
            if line_str.startswith('data: '):
                data_str = line_str[6:]
                if data_str == '[DONE]':
                    break
                try:
                    chunk = json.loads(data_str)
                    if 'choices' in chunk and chunk['choices']:
                        delta = chunk['choices'][0].get('delta', {})
                        if 'content' in delta:
                            content = delta['content']
                            print(content, end='', flush=True)
                            full_response += content
                    if 'thread_id' in chunk:
                        received_thread_id = chunk['thread_id']
                    if 'generated_files' in chunk:
                        generated_files.extend(chunk['generated_files'])
                except json.JSONDecodeError:
                    pass

    print()  # New line after streaming
    thread_id = received_thread_id
    print(f"ğŸ“ Response received with thread_id: {thread_id}")

    # Add assistant response to history
    conversation_history.append({"role": "assistant", "content": full_response})

    # Check for generated files
    if generated_files:
        print(f"ğŸ“ Files generated: {len(generated_files)}")

    # Second request with thread_id - generate analysis report
    print(f"\n2ï¸âƒ£ Second request - generating analysis report (with thread_id: {thread_id[:12] if thread_id else 'None'}...)...")
    conversation_history.append({
        "role": "user",
        "content": "åŸºäºåˆšæ‰çš„æ•°æ®ç»“æ„åˆ†æï¼Œè¯·ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æ•°æ®åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š\n1. æ•°æ®è´¨é‡è¯„ä¼°\n2. å„å­—æ®µçš„æ•°æ®åˆ†å¸ƒ\n3. ç›¸å…³æ€§åˆ†æ\n4. ä¸»è¦å‘ç°å’Œæ´å¯Ÿ"
    })
    if thread_id:
        conversation_history[-1]["thread_id"] = thread_id

    print("Streaming response...")
    response = requests.post(
        f"{API_BASE}/v1/chat/completions",
        json={
            "model": MODEL,
            "messages": conversation_history,
            "file_ids": [file_id],
            "temperature": 0.3,
            "stream": True
        },
        stream=True
    )

    if response.status_code != 200:
        print(f"âŒ Second request failed: {response.text}")
        return

    full_response2 = ""
    returned_thread_id = None
    generated_files2 = []

    for line in response.iter_lines():
        if line:
            line_str = line.decode('utf-8')
            if line_str.startswith('data: '):
                data_str = line_str[6:]
                if data_str == '[DONE]':
                    break
                try:
                    chunk = json.loads(data_str)
                    if 'choices' in chunk and chunk['choices']:
                        delta = chunk['choices'][0].get('delta', {})
                        if 'content' in delta:
                            content = delta['content']
                            print(content, end='', flush=True)
                            full_response2 += content
                    if 'thread_id' in chunk:
                        returned_thread_id = chunk['thread_id']
                    if 'generated_files' in chunk:
                        generated_files2.extend(chunk['generated_files'])
                except json.JSONDecodeError:
                    pass

    print()  # New line after streaming
    print(f"ğŸ“ Response thread_id: {returned_thread_id[:12] if returned_thread_id else 'None'}...")
    print(f"âœ… Thread ID match: {thread_id == returned_thread_id}")

    if generated_files2:
        print(f"ğŸ“ Files generated: {len(generated_files2)}")

    # Cleanup uploaded file
    requests.delete(f"{API_BASE}/v1/files/{file_id}")
    print("ğŸ—‘ï¸  Uploaded file cleaned up")

    print("\nâœ… 2-turn data analysis workflow completed successfully!")



def main():
    """Run examples"""
    print("ğŸš€ DeepAnalyze API Examples")
    print("API: localhost:8200 | Model: localhost:8000")

    examples = {
        "1": ("Simple Chat", simple_chat),
        "2": ("Chat with File", chat_with_file),
        "3": ("File IDs in Messages", file_ids_in_messages),
        "4": ("Streaming Chat", streaming_chat),
        "5": ("2-Turn Data Analysis", multi_turn_example),
        "6": ("All Examples", None)
    }

    while True:
        print("\nğŸ“‹ Examples:")
        for num, (name, _) in examples.items():
            print(f"{num}. {name}")
        print("0. Exit")

        choice = input("\nSelect (0-6): ").strip()

        if choice == "0":
            print("ğŸ‘‹ Goodbye!")
            break

        if choice not in examples:
            print("âŒ Invalid choice")
            continue

        try:
            if choice == "6":  # Run all examples
                for num, (name, func) in list(examples.items())[:-1]:
                    print(f"\n{name}:")
                    func()
            else:
                name, func = examples[choice]
                print(f"\n{name}:")
                func()

        except KeyboardInterrupt:
            print("\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"âŒ Error: {e}")
            print("Ensure API server (localhost:8200) and model server (localhost:8000) are running")


if __name__ == "__main__":
    main()
