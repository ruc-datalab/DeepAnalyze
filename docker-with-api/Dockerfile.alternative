# ============================================
# DeepAnalyze Docker Environment with API Support (Alternative)
# High-performance LLM inference with vLLM and CUDA 12.1
# Includes DeepAnalyze API Server and File Server
# ============================================

# Use a different base image with better stability
FROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1

# Set working directory
WORKDIR /workspace

# Install system dependencies with error handling
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    python3-pip \
    git \
    curl \
    wget \
    vim \
    ca-certificates \
    && apt-get clean && \
    rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

# Upgrade pip and install vLLM
RUN pip3 install --upgrade pip && \
    pip3 install vllm

# Install common AI/ML tools
RUN pip3 install \
    fastapi \
    uvicorn

# Install API server dependencies
RUN pip3 install \
    python-multipart \
    aiofiles \
    pydantic \
    python-dotenv \
    openai \
    requests \
    pandas \
    numpy

# Install data science and analysis tools
# Data processing and analysis
RUN pip3 install \
    pandasql \
    pyjanitor \
    feature-engine \
    missingno

# Data visualization
RUN pip3 install \
    plotly \
    bokeh \
    squarify \
    matplotlib-venn \
    wordcloud \
    folium \
    contextily

# Machine learning / Deep learning
RUN pip3 install \
    mlxtend \
    imbalanced-learn \
    optuna \
    shap \
    lime

# Time series and forecasting
RUN pip3 install \
    prophet \
    pmdarima \
    tslearn \
    lifelines

# Bayesian / Probabilistic modeling
RUN pip3 install \
    pymc \
    pystan \
    cmdstanpy \
    pyro-ppl

# Causal inference
RUN pip3 install \
    dowhy \
    econml \
    causalml \
    CausalInference \
    zepid

# Geospatial data science
RUN pip3 install \
    geopandas \
    shapely \
    rasterio \
    rasterstats \
    fiona \
    rtree \
    pysal \
    libpysal \
    esda \
    giddy \
    segregation \
    spopt \
    spreg \
    spglm \
    splot \
    spaghetti \
    tobler

# Create directories for API
RUN mkdir -p /workspace/API/workspace

# Copy API source code
COPY ./API /workspace/API/

# Expose ports for all services
EXPOSE 8000 8100 8200

# Create startup script
RUN cat > /workspace/start_services.sh << 'EOF'
#!/bin/bash
set -e

echo "ğŸš€ Starting DeepAnalyze Services..."

# Function to start vLLM server
start_vllm() {
    echo "ğŸ“‹ Starting vLLM server on port 8000..."
    # Check if model exists in /models or use default path
    if [ -d "/models/DeepAnalyze-8B" ]; then
        MODEL_PATH="/models/DeepAnalyze-8B"
        echo "Using model from /models/DeepAnalyze-8B"
    else
        MODEL_PATH="DeepAnalyze-8B"
        echo "Using model from HuggingFace or default path: DeepAnalyze-8B"
    fi

    python3 -m vllm.entrypoints.openai.api_server \
        --model ${MODEL_PATH} \
        --host 0.0.0.0 \
        --port 8000 \
        --trust-remote-code \
        --tensor-parallel-size 1 \
        --gpu-memory-utilization 0.9 &
    VLLM_PID=$!
    echo "vLLM server started with PID: $VLLM_PID"
}

# Function to start API server
start_api() {
    echo "ğŸ”§ Starting API server on port 8200..."
    cd /workspace/API

    # Set environment variables for API configuration
    export API_BASE="http://localhost:8000/v1"
    export MODEL_PATH="DeepAnalyze-8B"
    export WORKSPACE_BASE_DIR="workspace"

    python3 start_server.py &
    API_PID=$!
    echo "API server started with PID: $API_PID"
}

# Start vLLM server
start_vllm

# Wait a bit for vLLM to initialize
sleep 10

# Start API server
start_api

# Function to handle shutdown
cleanup() {
    echo "ğŸ›‘ Shutting down services..."
    if [ ! -z "$VLLM_PID" ]; then
        kill $VLLM_PID 2>/dev/null || true
    fi
    if [ ! -z "$API_PID" ]; then
        kill $API_PID 2>/dev/null || true
    fi
    echo "All services stopped."
    exit 0
}

# Set up signal handlers
trap cleanup SIGTERM SIGINT

# Monitor services
echo "âœ… All services started. Monitoring..."
echo "ğŸ“Š vLLM API: http://localhost:8000"
echo "ğŸ”§ API Server: http://localhost:8200"
echo "ğŸ“ File Server: http://localhost:8100"

# Keep the script running
wait
EOF

# Make startup script executable
RUN chmod +x /workspace/start_services.sh

# Default command to start all services
CMD ["/workspace/start_services.sh"]